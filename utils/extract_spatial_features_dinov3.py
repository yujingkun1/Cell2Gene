#!/usr/bin/env python3
"""
HESTç»†èƒæ·±åº¦ç‰¹å¾æå–å™¨
ä½¿ç”¨DINOv2æå–æ¯ä¸ªç»†èƒçš„æ·±åº¦ç‰¹å¾ï¼Œç»“åˆå½¢æ€ç‰¹å¾æ„å»ºç»¼åˆç‰¹å¾å‘é‡
"""

import os
import pandas as pd
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image, ImageDraw
import cv2
from sklearn.decomposition import PCA
import pickle
from tqdm import tqdm
import warnings
import shapely.wkb as wkb
import h5py
import json
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
import gc
warnings.filterwarnings("ignore")

# è®¾ç½®HuggingFaceé•œåƒï¼ˆç”¨äºä¸­å›½ç½‘ç»œç¯å¢ƒï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
print(f"å·²è®¾ç½®HuggingFaceé•œåƒ: {os.environ.get('HF_ENDPOINT')}")

# DINOv3æ¨¡å‹å¯¼å…¥
try:
    import transformers
    from transformers import AutoModel, AutoImageProcessor
    DINOV3_AVAILABLE = True
    print("âœ“ DINOv3å¯ç”¨")
except ImportError as e:
    DINOV3_AVAILABLE = False
    print(f"é”™è¯¯: è¯·å®‰è£…transformersåº“: pip install transformers")
    print(f"è¯¦ç»†é”™è¯¯: {e}")

class HESTCellFeatureExtractor:
    """HESTç»†èƒæ·±åº¦ç‰¹å¾æå–å™¨ï¼ˆDINOv3ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, 
                 hest_data_dir,
                 output_dir,
                 dinov3_model_path=None,      # DINOv3æœ¬åœ°æ¨¡å‹è·¯å¾„
                 bulk_pca_model_path=None,     # å·²ç§»é™¤ï¼Œä»…ä¿ç•™ç‹¬ç«‹PCA
                 cell_patch_size=48,           # ç»†èƒpatchå¤§å°ï¼ŒåŸºäºå®é™…ç»†èƒå¤§å°åˆ†æ
                 dinov3_feature_dim=1024,     # DINOv3-Lç‰¹å¾ç»´åº¦
                 final_dino_dim=128,          # PCAé™ç»´åDINOç»´åº¦ï¼ˆç»Ÿä¸€ä¸º128ï¼‰
                 device='cuda',
                 dino_batch_size=256,         # å¤§å¹…å¢åŠ DINOv3æ‰¹å¤„ç†å¤§å°
                 cell_batch_size=50000,       # å¤§å¹…å¢åŠ ç»†èƒæ‰¹å¤„ç†å¤§å°
                 num_workers=8):              # å¤šè¿›ç¨‹å·¥ä½œè€…æ•°é‡
        
        if not DINOV3_AVAILABLE:
            raise ImportError("DINOv3ä¸å¯ç”¨ï¼Œè¯·å®‰è£…transformers")
            
        self.hest_data_dir = hest_data_dir
        self.output_dir = output_dir
        self.dinov3_model_path = dinov3_model_path
        self.bulk_pca_model_path = bulk_pca_model_path
        self.cell_patch_size = cell_patch_size
        self.dinov3_feature_dim = dinov3_feature_dim
        self.final_dino_dim = final_dino_dim
        self.final_feature_dim = final_dino_dim  # åªä½¿ç”¨DINOv3ç‰¹å¾ï¼Œ128ç»´
        self.device = device
        self.dino_batch_size = dino_batch_size
        self.cell_batch_size = cell_batch_size
        self.num_workers = num_workers
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–DINOv3æ¨¡å‹
        self.init_dinov3_model()
        
        
        print(f"è¿ç§»å­¦ä¹ é…ç½®:")
        print(f"  - DINOv3æ‰¹å¤„ç†å¤§å°: {self.dino_batch_size}")
        print(f"  - ç»†èƒæ‰¹å¤„ç†å¤§å°: {self.cell_batch_size}")
        print(f"  - å¤šè¿›ç¨‹å·¥ä½œè€…: {self.num_workers}")
        print(f"  - è®¾å¤‡: {self.device}")
        print(f"  - æœ€ç»ˆç‰¹å¾ç»´åº¦: {self.final_feature_dim} (ä»…DINOv3+PCA)")
        print(f"  - æ¨¡å‹ä¸€è‡´æ€§: ä½¿ç”¨DINOv3-L")
        
    def init_dinov3_model(self):
        """åˆå§‹åŒ–DINOv3æ¨¡å‹ï¼ˆä½¿ç”¨torch.hubåŠ è½½æœ¬åœ°æƒé‡ï¼‰"""
        print("åˆå§‹åŒ–DINOv3æ¨¡å‹...")
        
        if not self.dinov3_model_path or not os.path.exists(self.dinov3_model_path):
            raise RuntimeError(f"DINOv3æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.dinov3_model_path}")
        
        print(f"ä½¿ç”¨æœ¬åœ°DINOv3æ¨¡å‹: {self.dinov3_model_path}")
        
        try:
            # è®¾ç½® DINOv3 ä»“åº“è·¯å¾„
            dinov3_repo_dir = "/data/yujk/hovernet2feature/dinov3"
            
            if not os.path.exists(dinov3_repo_dir):
                raise RuntimeError(f"DINOv3ä»“åº“ä¸å­˜åœ¨: {dinov3_repo_dir}")
            
            print(f"ä½¿ç”¨DINOv3ä»“åº“: {dinov3_repo_dir}")
            
            # ä½¿ç”¨ torch.hub.load åŠ è½½ DINOv3 ViT-L/16 æ¨¡å‹
            print("ä½¿ç”¨torch.hubåŠ è½½DINOv3-ViT-L/16æ¨¡å‹...")
            
            # ç›´æ¥ä½¿ç”¨torch.hubåŠ è½½
            self.dino_model = torch.hub.load(
                dinov3_repo_dir, 
                'dinov3_vitl16',  # DINOv3 ViT-L/16 æ¨¡å‹
                source='local',
                weights=self.dinov3_model_path,  # ä½¿ç”¨æœ¬åœ°æƒé‡
                trust_repo=True
            )
            
            print("âœ“ æˆåŠŸä½¿ç”¨torch.hubåŠ è½½DINOv3æ¨¡å‹")
            
            # è®¾ç½®ç‰¹å¾ç»´åº¦
            self.dinov3_feature_dim = 1024  # DINOv3-L çš„ç‰¹å¾ç»´åº¦
            
            # è®¾ç½®å›¾åƒå¤„ç†å™¨ï¼ˆä½¿ç”¨æ ‡å‡†çš„ImageNeté¢„å¤„ç†ï¼‰
            from torchvision import transforms
            
            self.dino_processor_transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                )
            ])
            
            print("âœ“ è®¾ç½®å›¾åƒé¢„å¤„ç†å™¨")
            
        except Exception as e:
            print(f"torch.hubåŠ è½½å¤±è´¥: {e}")
            
            # å¤‡é€‰æ–¹æ¡ˆï¼šæ‰‹åŠ¨å®ç°DINOv3åŠ è½½
            print("å°è¯•æ‰‹åŠ¨åŠ è½½...")
            try:
                # ç›´æ¥åŠ è½½æ¨¡å‹æ–‡ä»¶
                checkpoint = torch.load(self.dinov3_model_path, map_location='cpu')
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æ¨¡å‹å¯¹è±¡
                if hasattr(checkpoint, 'forward'):
                    # ç›´æ¥æ˜¯æ¨¡å‹å¯¹è±¡
                    self.dino_model = checkpoint
                    print("âœ“ ç›´æ¥åŠ è½½æ¨¡å‹å¯¹è±¡")
                else:
                    # æ˜¯çŠ¶æ€å­—å…¸ï¼Œéœ€è¦å…ˆå»ºç«‹æ¨¡å‹æ¶æ„
                    print("æ£€æµ‹åˆ°çŠ¶æ€å­—å…¸ï¼Œæ­£åœ¨å»ºç«‹æ¨¡å‹æ¶æ„...")
                    
                    # å°è¯•ä½¿ç”¨timmæˆ–æ‰‹åŠ¨å»ºç«‹æ¨¡å‹æ¶æ„
                    try:
                        import timm
                        # ä½¿ç”¨timmåˆ›å»½DINOv3ç±»ä¼¼çš„æ¨¡å‹
                        self.dino_model = timm.create_model(
                            'vit_large_patch16_224',
                            pretrained=False,
                            num_classes=0,  # åªè¦ç‰¹å¾æå–
                            global_pool=''
                        )
                        
                        # å°è¯•åŠ è½½æƒé‡
                        if isinstance(checkpoint, dict):
                            if 'model' in checkpoint:
                                state_dict = checkpoint['model']
                            elif 'state_dict' in checkpoint:
                                state_dict = checkpoint['state_dict']
                            else:
                                state_dict = checkpoint
                        else:
                            state_dict = checkpoint
                        
                        # å°è¯•åŠ è½½çŠ¶æ€å­—å…¸
                        missing_keys, unexpected_keys = self.dino_model.load_state_dict(state_dict, strict=False)
                        print(f"âœ“ ä½¿ç”¨timmåŠ è½½æ¨¡å‹ï¼Œç¼ºå°‘: {len(missing_keys)}, æ„å¤–: {len(unexpected_keys)}")
                        
                    except ImportError:
                        print("timmä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install timm")
                        raise RuntimeError("æ— æ³•åŠ è½½DINOv3æ¨¡å‹")
                
                # è®¾ç½®å›¾åƒå¤„ç†å™¨
                from torchvision import transforms
                self.dino_processor_transform = transforms.Compose([
                    transforms.Resize((224, 224)),
                    transforms.ToTensor(),
                    transforms.Normalize(
                        mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225]
                    )
                ])
                
                self.dinov3_feature_dim = 1024
                
            except Exception as e2:
                raise RuntimeError(f"æ‰€æœ‰DINOv3åŠ è½½æ–¹å¼éƒ½å¤±è´¥:\ntorch.hub: {e}\næ‰‹åŠ¨åŠ è½½: {e2}")
        
        # è®¾ç½®æ¨¡å‹å±æ€§
        self.dino_model.to(self.device)
        
        # ä¸å¼ºåˆ¶è®¾ç½®ä¸ºFP16ï¼Œè®©autocastè‡ªåŠ¨ç®¡ç†ç²¾åº¦
        # if torch.cuda.is_available():
        #     self.dino_model = self.dino_model.half()  # æ³¨é‡Šæ‰å¼ºåˆ¶FP16
        
        self.dino_model.eval()
        
        # é¢„çƒ­GPU
        if torch.cuda.is_available():
            dummy_input = torch.randn(1, 3, 224, 224, device=self.device, dtype=torch.float32)  # ä½¿ç”¨FP32é¢„çƒ­
            with torch.no_grad():
                try:
                    _ = self.dino_model(dummy_input)
                    print("  âœ“ GPUé¢„çƒ­æˆåŠŸ")
                except Exception as e:
                    print(f"  è­¦å‘Š: GPUé¢„çƒ­å¤±è´¥: {e}")
            torch.cuda.empty_cache()
        
        print(f"âœ“ DINOv3æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç‰¹å¾ç»´åº¦: {self.dinov3_feature_dim}")
    
    def monitor_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory = psutil.virtual_memory()
            
            resource_info = f"CPU: {cpu_percent:.1f}%, RAM: {memory.percent:.1f}%"
            
            if torch.cuda.is_available():
                gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                gpu_percent = (gpu_memory_used / gpu_memory_total) * 100
                resource_info += f", GPU: {gpu_percent:.1f}% ({gpu_memory_used:.2f}/{gpu_memory_total:.1f}GB)"
            
            return resource_info
        except ImportError:
            return "èµ„æºç›‘æ§éœ€è¦å®‰è£…psutil: pip install psutil"
    
    def load_sample_data(self, sample_id):
        """åŠ è½½å•ä¸ªæ ·æœ¬çš„æ•°æ®"""
        print(f"åŠ è½½æ ·æœ¬æ•°æ®: {sample_id}")
        
        sample_data = {}
        
        # åŠ è½½WSIå›¾åƒ
        wsi_path = os.path.join(self.hest_data_dir, "wsis", f"{sample_id}.tif")
        if not os.path.exists(wsi_path):
            raise FileNotFoundError(f"WSIæ–‡ä»¶ä¸å­˜åœ¨: {wsi_path}")
        
        # åŠ è½½ç»†èƒåˆ†å‰²æ•°æ®
        cellvit_path = os.path.join(self.hest_data_dir, "cellvit_seg", f"{sample_id}_cellvit_seg.parquet")
        if not os.path.exists(cellvit_path):
            raise FileNotFoundError(f"ç»†èƒåˆ†å‰²æ–‡ä»¶ä¸å­˜åœ¨: {cellvit_path}")
        
        cellvit_df = pd.read_parquet(cellvit_path)
        
        sample_data = {
            'wsi_path': wsi_path,
            'cellvit_df': cellvit_df,
            'sample_id': sample_id
        }
        
        print(f"  WSI: {wsi_path}")
        print(f"  ç»†èƒæ•°é‡: {len(cellvit_df)}")
        
        return sample_data
    
    def extract_cell_patch(self, wsi_image, cell_geometry, patch_size=None):
        """ä»WSIä¸­æå–å•ä¸ªç»†èƒçš„å›¾åƒpatch"""
        if patch_size is None:
            patch_size = self.cell_patch_size
        
        try:
            # è§£æç»†èƒå‡ ä½•å½¢çŠ¶
            if isinstance(cell_geometry, bytes):
                geom = wkb.loads(cell_geometry)
            else:
                geom = cell_geometry
            
            # è·å–ç»†èƒè¾¹ç•Œæ¡†
            bounds = geom.bounds  # (minx, miny, maxx, maxy)
            
            # è®¡ç®—ç»†èƒä¸­å¿ƒç‚¹å’ŒpatchåŒºåŸŸ
            center_x = (bounds[0] + bounds[2]) / 2
            center_y = (bounds[1] + bounds[3]) / 2
            
            # å®šä¹‰patchåŒºåŸŸ
            half_size = patch_size // 2
            x1 = max(0, int(center_x - half_size))
            y1 = max(0, int(center_y - half_size))
            x2 = min(wsi_image.shape[1], int(center_x + half_size))
            y2 = min(wsi_image.shape[0], int(center_y + half_size))
            
            # æå–patch
            cell_patch = wsi_image[y1:y2, x1:x2]
            
            # å¦‚æœpatchå¤§å°ä¸å¤Ÿï¼Œè¿›è¡Œå¡«å……
            if cell_patch.shape[0] < patch_size or cell_patch.shape[1] < patch_size:
                padded_patch = np.zeros((patch_size, patch_size, 3), dtype=cell_patch.dtype)
                h, w = cell_patch.shape[:2]
                padded_patch[:h, :w] = cell_patch
                cell_patch = padded_patch
            
            # å¦‚æœpatchå¤ªå¤§ï¼Œè¿›è¡Œè£å‰ª
            elif cell_patch.shape[0] > patch_size or cell_patch.shape[1] > patch_size:
                cell_patch = cell_patch[:patch_size, :patch_size]
            
            return cell_patch
            
        except Exception as e:
            print(f"æå–ç»†èƒpatchå¤±è´¥: {e}")
            # è¿”å›ç©ºç™½patch
            return np.zeros((patch_size, patch_size, 3), dtype=np.uint8)
    
    def extract_dino_features(self, cell_patches):
        """ä½¿ç”¨DINOv3æå–ç»†èƒpatchesçš„ç‰¹å¾ï¼ˆé«˜æ€§èƒ½æ‰¹é‡å¤„ç†ï¼‰"""
        if len(cell_patches) == 0:
            return np.zeros((0, self.dinov3_feature_dim))
        
        print(f"å¼€å§‹æå– {len(cell_patches)} ä¸ªpatchesçš„DINOv3ç‰¹å¾...")
        print(f"ä½¿ç”¨æ‰¹å¤„ç†å¤§å°: {self.dino_batch_size} (å¤§å¹…ä¼˜åŒ–GPUåˆ©ç”¨ç‡)")
        
        features = []
        batch_size = self.dino_batch_size  # ä½¿ç”¨æ›´å¤§çš„æ‰¹å¤„ç†å¤§å°
        
        # åˆ†æ‰¹å¤„ç†æ‰€æœ‰patches
        total_batches = (len(cell_patches) + batch_size - 1) // batch_size
        
        with torch.no_grad():
            for i in tqdm(range(0, len(cell_patches), batch_size), desc="DINOv3ç‰¹å¾æå–", total=total_batches):
                batch_patches = cell_patches[i:i+batch_size]
                
                # DINOv3å¤„ç†ï¼ˆä¼˜åŒ–å†…å­˜å’Œè®¡ç®—ï¼‰
                try:
                    # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰çš„é¢„å¤„ç†å™¨
                    if hasattr(self, 'dino_processor_transform'):
                        # ä½¿ç”¨è‡ªå®šä¹‰çš„torchvisioné¢„å¤„ç†å™¨
                        processed_tensors = []
                        for patch in batch_patches:
                            # ç¡®ä¿æ˜¯0-255çš„åƒç´ å€¼
                            if patch.max() <= 1.0:
                                patch = (patch * 255).astype(np.uint8)
                            
                            # è½¬æ¢ä¸ºPILå›¾åƒ
                            from PIL import Image
                            pil_image = Image.fromarray(patch)
                            
                            # åº”ç”¨é¢„å¤„ç†
                            tensor = self.dino_processor_transform(pil_image)
                            processed_tensors.append(tensor)
                        
                        # å †å æˆbatch
                        batch_tensor = torch.stack(processed_tensors)
                        
                    else:
                        # ä½¿ç”¨HuggingFaceçš„å¤„ç†å™¨
                        processed_images = self._parallel_preprocess_images(batch_patches)
                        inputs = self.dino_processor(images=processed_images, return_tensors="pt")
                        batch_tensor = inputs['pixel_values']
                    
                    # ç§»åŠ¨åˆ°è®¾å¤‡ï¼Œè®©autocastç®¡ç†ç²¾åº¦
                    batch_tensor = batch_tensor.to(self.device, non_blocking=True)
                    # ä¸å¼ºåˆ¶è½¬æ¢ä¸ºFP16ï¼Œè®©autocastè‡ªåŠ¨ç®¡ç†
                    # if torch.cuda.is_available():
                    #     batch_tensor = batch_tensor.to(self.device, non_blocking=True, dtype=torch.float16)
                    # else:
                    #     batch_tensor = batch_tensor.to(self.device, non_blocking=True)
                    
                    # æå–ç‰¹å¾ï¼ˆå¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰
                    if torch.cuda.is_available():
                        with torch.amp.autocast('cuda'):  # æ›´æ–°çš„autocastè¯­æ³•
                            if hasattr(self, 'dino_processor_transform'):
                                # torch.hubåŠ è½½çš„æ¨¡å‹ï¼Œç›´æ¥è°ƒç”¨
                                batch_features = self.dino_model(batch_tensor)
                                # å¦‚æœè¿”å›çš„æ˜¯tupleï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                                if isinstance(batch_features, tuple):
                                    batch_features = batch_features[0]
                                # å¦‚æœæ˜¯4D tensor (batch, channels, height, width)ï¼Œå–global average pooling
                                if len(batch_features.shape) == 4:
                                    batch_features = batch_features.mean(dim=[2, 3])  # Global average pooling
                                elif len(batch_features.shape) == 3:
                                    batch_features = batch_features.mean(dim=1)  # å¹³å‡tokenç‰¹å¾
                            else:
                                # HuggingFaceæ¨¡å‹
                                outputs = self.dino_model(pixel_values=batch_tensor)
                                batch_features = outputs.last_hidden_state.mean(dim=1)
                    else:
                        if hasattr(self, 'dino_processor_transform'):
                            batch_features = self.dino_model(batch_tensor)
                            if isinstance(batch_features, tuple):
                                batch_features = batch_features[0]
                            if len(batch_features.shape) == 4:
                                batch_features = batch_features.mean(dim=[2, 3])
                            elif len(batch_features.shape) == 3:
                                batch_features = batch_features.mean(dim=1)
                        else:
                            outputs = self.dino_model(pixel_values=batch_tensor)
                            batch_features = outputs.last_hidden_state.mean(dim=1)
                    
                    # è½¬æ¢ä¸ºnumpyå¹¶æ£€æŸ¥NaN/Infå€¼
                    batch_features_np = batch_features.cpu().numpy()
                    
                    # æ£€æŸ¥å¹¶æ¸…ç†NaN/Infå€¼
                    if np.isnan(batch_features_np).any() or np.isinf(batch_features_np).any():
                        print(f"  è­¦å‘Š: æ‰¹æ¬¡ {i//batch_size} æ£€æµ‹åˆ°NaN/Infå€¼ï¼Œè¿›è¡Œæ¸…ç†")
                        batch_features_np = np.nan_to_num(batch_features_np, nan=0.0, posinf=1.0, neginf=-1.0)
                    
                    features.append(batch_features_np)
                    
                    # å®æ—¶èµ„æºç›‘æ§
                    if i % (batch_size * 5) == 0:  # æ¯5ä¸ªå¤§æ‰¹æ¬¡æ˜¾ç¤ºèµ„æºä½¿ç”¨
                        resource_info = self.monitor_resources()
                        print(f"  èµ„æºä½¿ç”¨: {resource_info}")
                    
                    # å‡å°‘å†…å­˜æ¸…ç†é¢‘ç‡ï¼Œæé«˜æ•ˆç‡
                    if i % (batch_size * 20) == 0:  # æ¯20ä¸ªå¤§æ‰¹æ¬¡æ¸…ç†ä¸€æ¬¡
                        torch.cuda.empty_cache()
                        
                except Exception as e:
                    print(f"æ‰¹æ¬¡ {i//batch_size} DINOv3å¤„ç†å¤±è´¥: {e}")
                    # ä½¿ç”¨é›¶å‘é‡æ›¿ä»£
                    zero_features = np.zeros((len(batch_patches), self.dinov3_feature_dim))
                    features.append(zero_features)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        all_features = np.vstack(features) if features else np.zeros((0, self.dinov3_feature_dim))
        
        print(f"DINOv3ç‰¹å¾æå–å®Œæˆ: {all_features.shape}")
        return all_features
    
    def _parallel_preprocess_images(self, batch_patches):
        """å¹¶è¡Œé¢„å¤„ç†å›¾åƒpatches"""
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            processed_images = list(executor.map(self._preprocess_single_image, batch_patches))
        return processed_images
    
    def _preprocess_single_image(self, patch):
        """é¢„å¤„ç†å•ä¸ªå›¾åƒpatch"""
        if patch.max() <= 1.0:
            patch = (patch * 255).astype(np.uint8)
        return Image.fromarray(patch)
    
    
    
    
    
    def load_wsi_image(self, wsi_path):
        """åŠ è½½WSIå›¾åƒ"""
        try:
            # å°è¯•ä½¿ç”¨openslideåŠ è½½WSI
            try:
                import openslide
                wsi = openslide.OpenSlide(wsi_path)
                # è·å–æœ€é«˜åˆ†è¾¨ç‡çº§åˆ«
                level = 0
                wsi_image = wsi.read_region((0, 0), level, wsi.level_dimensions[level])
                wsi_image = np.array(wsi_image.convert('RGB'))
                print(f"  ä½¿ç”¨openslideåŠ è½½WSI: {wsi_image.shape}")
                return wsi_image
            except ImportError:
                print("  openslideä¸å¯ç”¨ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                
            # å°è¯•ä½¿ç”¨cv2åŠ è½½
            import cv2
            wsi_image = cv2.imread(wsi_path)
            if wsi_image is not None:
                wsi_image = cv2.cvtColor(wsi_image, cv2.COLOR_BGR2RGB)
                print(f"  ä½¿ç”¨cv2åŠ è½½WSI: {wsi_image.shape}")
                return wsi_image
            
            # å°è¯•ä½¿ç”¨PILåŠ è½½
            from PIL import Image
            wsi_image = Image.open(wsi_path).convert('RGB')
            wsi_image = np.array(wsi_image)
            print(f"  ä½¿ç”¨PILåŠ è½½WSI: {wsi_image.shape}")
            return wsi_image
            
        except Exception as e:
            print(f"  åŠ è½½WSIå¤±è´¥: {e}")
            return None

    def process_sample_with_independent_pca(self, sample_id):
        """å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œç‹¬ç«‹è®­ç»ƒPCAï¼Œæå–å®Œç›´æ¥ä¿å­˜ï¼ˆæ¯ä¾‹ç‹¬ç«‹PCAç‰ˆæœ¬ï¼‰"""
        print(f"\n=== å¤„ç†ç©ºè½¬æ ·æœ¬: {sample_id} ===")
        print("æ¯ä¾‹ç‹¬ç«‹è®­ç»ƒPCAæ¨¡å‹ï¼Œ128ç»´DINOv2ç‰¹å¾")
        
        # åŠ è½½æ ·æœ¬æ•°æ®
        sample_data = self.load_sample_data(sample_id)
        cellvit_df = sample_data['cellvit_df']
        wsi_path = sample_data['wsi_path']
        
        # æå–ç»†èƒpatches
        num_cells = len(cellvit_df)
        max_cells = num_cells  # æå–æ‰€æœ‰ç»†èƒï¼Œä¸é™åˆ¶æ•°é‡
        cell_patches = []
        
        print(f"å‡†å¤‡æå–æ‰€æœ‰ {num_cells} ä¸ªç»†èƒçš„ç‰¹å¾...")
        
        # å…ˆå°è¯•åŠ è½½WSIå›¾åƒçš„å°åŒºåŸŸæ¥æµ‹è¯•
        print("å°è¯•åŠ è½½WSIå›¾åƒ...")
        try:
            import openslide
            wsi = openslide.OpenSlide(wsi_path)
            print(f"  WSIå°ºå¯¸: {wsi.dimensions}")
            print(f"  WSIçº§åˆ«æ•°: {wsi.level_count}")
            print(f"  WSIçº§åˆ«å°ºå¯¸: {wsi.level_dimensions}")
            
            # ä½¿ç”¨çº§åˆ«1åˆ†è¾¨ç‡ï¼Œæä¾›åˆé€‚çš„ç»†èƒè¦†ç›–èŒƒå›´
            level = 1  # ä½¿ç”¨çº§åˆ«1ï¼Œ~0.5Î¼m/pixelï¼Œ48x48åƒç´ è¦†ç›–24x24Î¼m
            print(f"  ä½¿ç”¨çº§åˆ« {level}ï¼Œå°ºå¯¸: {wsi.level_dimensions[level]}")
            
            # ä»çœŸå®WSIä¸­æå–ç»†èƒpatchesï¼ˆå¤§æ‰¹é‡å¹¶è¡Œå¤„ç†ï¼‰
            batch_size = self.cell_batch_size  # ä½¿ç”¨ä¼˜åŒ–åçš„æ‰¹å¤„ç†å¤§å°
            num_batches = (max_cells + batch_size - 1) // batch_size
            
            print(f"  å°†åˆ† {num_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} ä¸ªç»†èƒ (å¤§å¹…ä¼˜åŒ–å¤„ç†æ•ˆç‡)")
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, max_cells)
                batch_cells = end_idx - start_idx
                
                print(f"\n  å¤„ç†æ‰¹æ¬¡ {batch_idx+1}/{num_batches}: ç»†èƒ {start_idx}-{end_idx-1} ({batch_cells} ä¸ª)")
                
                # å¹¶è¡Œæå–patchesä»¥å¤§å¹…æé«˜CPUåˆ©ç”¨ç‡
                print(f"\n  ä½¿ç”¨{self.num_workers}ä¸ªå¹¶è¡Œè¿›ç¨‹æå–patches...")
                batch_patches = self._extract_patches_parallel(
                    cellvit_df.iloc[start_idx:end_idx], wsi, level, start_idx
                )
                
                # å°†æœ¬æ‰¹æ¬¡çš„patchesæ·»åŠ åˆ°æ€»åˆ—è¡¨
                cell_patches.extend(batch_patches)
                
                # ä¼˜åŒ–å†…å­˜æ¸…ç†
                del batch_patches
                gc.collect()
                
                print(f"  æ‰¹æ¬¡ {batch_idx+1} å®Œæˆï¼Œç´¯è®¡æå– {len(cell_patches)} ä¸ªpatches")
                
                # å®æ—¶GPUå†…å­˜çŠ¶æ€ç›‘æ§
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated() / 1024**3
                    print(f"  å½“å‰GPUå†…å­˜ä½¿ç”¨: {gpu_memory:.2f} GB")
            
            wsi.close()
            print(f"  æˆåŠŸä»WSIæå– {len(cell_patches)} ä¸ªçœŸå®ç»†èƒpatches")
            
        except Exception as e:
            print(f"  WSIåŠ è½½å¤±è´¥: {e}")
            print("  æ— æ³•ä»WSIä¸­æå–çœŸå®ç»†èƒå›¾åƒï¼Œç¨‹åºç»ˆæ­¢")
            raise RuntimeError(f"WSIå›¾åƒåŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ç‰¹å¾æå–: {e}") from e
        
        # æå–DINOv2ç‰¹å¾
        print("æå–DINOv2ç‰¹å¾...")
        dino_features = self.extract_dino_features(cell_patches)
        
        # ä¸ºå½“å‰æ ·æœ¬ç‹¬ç«‹è®­ç»ƒPCAé™ç»´ï¼ˆä¸èåˆå½¢æ€ç‰¹å¾ï¼‰
        print(f"ä¸ºæ ·æœ¬ {sample_id} ç‹¬ç«‹è®­ç»ƒPCAé™ç»´...")
        final_features = self.apply_independent_pca(dino_features, sample_id)
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            'sample_id': sample_id,
            'num_cells': len(cell_patches),
            'feature_dim': self.final_feature_dim,
            'dino_dim': self.final_dino_dim,
            'patch_size': self.cell_patch_size,
            'wsi_level': 1,  # ä½¿ç”¨çº§åˆ«1
            'total_cells_processed': len(cell_patches),
            'independent_pca': True,
            'pca_trained_on_sample': sample_id
        }
        
        # ä¿å­˜ç‰¹å¾
        output_file = self.save_features(sample_id, final_features, metadata)
        
        print(f"\n=== æ€§èƒ½ç»Ÿè®¡ ===")
        total_cells = len(cell_patches)
        print(f"âœ“ æ€»å¤„ç†ç»†èƒæ•°: {total_cells:,}")
        print(f"âœ“ DINOv2æ‰¹å¤„ç†å¤§å°: {self.dino_batch_size}")
        print(f"âœ“ ç»†èƒæ‰¹å¤„ç†å¤§å°: {self.cell_batch_size}")
        print(f"âœ“ å¹¶è¡Œå·¥ä½œè€…æ•°: {self.num_workers}")
        print(f"âœ“ æœ€ç»ˆç‰¹å¾ç»´åº¦: {self.final_feature_dim} (æ ·æœ¬ç‹¬ç«‹PCA)")
        print(f"âœ“ ç‰¹å¾æ–‡ä»¶: {output_file}")
        if torch.cuda.is_available():
            print(f"âœ“ æœ€ç»ˆGPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        return {
            'sample_id': sample_id,
            'num_cells': len(cell_patches),
            'final_feature_dim': self.final_feature_dim,
            'output_file': output_file
        }
    
    def _extract_patches_parallel(self, cellvit_batch, wsi, level, start_idx):
        """ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œæå–ç»†èƒpatchesä»¥æœ€å¤§åŒ–CPUåˆ©ç”¨ç‡"""
        batch_patches = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è€Œä¸æ˜¯è¿›ç¨‹æ± ï¼Œå› ä¸ºWSIå¯¹è±¡ä¸èƒ½åºåˆ—åŒ–
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
            tasks = []
            for i, (_, row) in enumerate(cellvit_batch.iterrows()):
                task = executor.submit(
                    self._extract_single_patch_threaded,
                    row['geometry'], 
                    wsi, 
                    level, 
                    self.cell_patch_size,
                    start_idx + i
                )
                tasks.append(task)
            
            # æ”¶é›†ç»“æœ
            for task in tqdm(tasks, desc=f"æ‰¹æ¬¡å¹¶è¡Œæå–patches"):
                try:
                    patch = task.result(timeout=10)  # 10ç§’è¶…æ—¶
                    batch_patches.append(patch)
                except Exception as e:
                    print(f"  å¹¶è¡Œæå–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤patch: {e}")
                    batch_patches.append(np.zeros((self.cell_patch_size, self.cell_patch_size, 3), dtype=np.uint8))
        
        return batch_patches
    
    def _extract_single_patch_threaded(self, geom_bytes, wsi, level, patch_size, cell_idx):
        """çº¿ç¨‹å®‰å…¨çš„å•ä¸ªç»†èƒpatchæå–ï¼Œä½¿ç”¨é»‘è‰²å¡«å……æ— æ•ˆåŒºåŸŸ"""
        try:
            geom = wkb.loads(geom_bytes)
            centroid = geom.centroid
            center_x, center_y = centroid.x, centroid.y
            
            # è®¡ç®—åœ¨æŒ‡å®šçº§åˆ«ä¸‹çš„åæ ‡
            scale_factor = wsi.level_downsamples[level]
            half_size = patch_size // 2
            
            # è®¡ç®—WSIè¾¹ç•Œ
            wsi_width, wsi_height = wsi.level_dimensions[level]
            
            # è®¡ç®—æå–åŒºåŸŸåæ ‡
            x_start = int(center_x - half_size * scale_factor)
            y_start = int(center_y - half_size * scale_factor)
            x_end = x_start + int(patch_size * scale_factor)
            y_end = y_start + int(patch_size * scale_factor)
            
            # åˆ›å»ºé»‘è‰²å¡«å……çš„patch
            cell_patch = np.zeros((patch_size, patch_size, 3), dtype=np.uint8)
            
            # è®¡ç®—WSIå†…çš„æœ‰æ•ˆåŒºåŸŸ
            valid_x_start = max(0, x_start)
            valid_y_start = max(0, y_start)
            valid_x_end = min(wsi_width * scale_factor, x_end)
            valid_y_end = min(wsi_height * scale_factor, y_end)
            
            # å¦‚æœæœ‰æœ‰æ•ˆåŒºåŸŸï¼Œæå–è¯¥éƒ¨åˆ†
            if valid_x_start < valid_x_end and valid_y_start < valid_y_end:
                # è®¡ç®—åœ¨WSIä¸­çš„å®é™…æå–å°ºå¯¸
                region_width = int((valid_x_end - valid_x_start) / scale_factor)
                region_height = int((valid_y_end - valid_y_start) / scale_factor)
                
                if region_width > 0 and region_height > 0:
                    # æå–æœ‰æ•ˆåŒºåŸŸ
                    region = wsi.read_region(
                        (valid_x_start, valid_y_start),
                        level,
                        (region_width, region_height)
                    )
                    
                    # è½¬æ¢ä¸ºRGBæ•°ç»„
                    region_array = np.array(region.convert('RGB'))
                    
                    # è®¡ç®—åœ¨patchä¸­çš„æ”¾ç½®ä½ç½®
                    patch_x_start = max(0, int((valid_x_start - x_start) / scale_factor))
                    patch_y_start = max(0, int((valid_y_start - y_start) / scale_factor))
                    patch_x_end = min(patch_size, patch_x_start + region_array.shape[1])
                    patch_y_end = min(patch_size, patch_y_start + region_array.shape[0])
                    
                    # ç¡®ä¿ä¸è¶…å‡ºè¾¹ç•Œï¼Œå¹¶å°†æœ‰æ•ˆåŒºåŸŸæ”¾å…¥patch
                    if (patch_x_end > patch_x_start and patch_y_end > patch_y_start and 
                        region_array.shape[0] > 0 and region_array.shape[1] > 0):
                        
                        # è°ƒæ•´region_arrayå¤§å°ä»¥åŒ¹é…ç›®æ ‡åŒºåŸŸ
                        target_height = patch_y_end - patch_y_start
                        target_width = patch_x_end - patch_x_start
                        
                        if region_array.shape[:2] != (target_height, target_width):
                            region_array = cv2.resize(region_array, (target_width, target_height))
                        
                        cell_patch[patch_y_start:patch_y_end, patch_x_start:patch_x_end] = region_array
            
            return cell_patch
            
        except Exception as e:
            if cell_idx < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"    ç»†èƒ {cell_idx} patchæå–å¤±è´¥: {e}")
            # è¿”å›çº¯é»‘è‰²patchä½œä¸ºé»˜è®¤å€¼
            return np.zeros((patch_size, patch_size, 3), dtype=np.uint8)
    
    def apply_independent_pca(self, dino_features, sample_id):
        """ä¸ºå½“å‰æ ·æœ¬ç‹¬ç«‹è®­ç»ƒPCAé™ç»´ï¼ˆæ¯ä¾‹ç‹¬ç«‹PCAç‰ˆæœ¬ï¼‰"""
        print(f"ä¸ºæ ·æœ¬ {sample_id} ç‹¬ç«‹è®­ç»ƒPCAé™ç»´å™¨...")
        
        # æ£€æŸ¥å¹¶æ¸…ç†NaNå€¼
        print(f"  æ£€æŸ¥æ•°æ®è´¨é‡...")
        nan_count = np.isnan(dino_features).sum()
        inf_count = np.isinf(dino_features).sum()
        
        if nan_count > 0:
            print(f"  è­¦å‘Š: å‘ç° {nan_count} ä¸ªNaNå€¼ï¼Œå°†è¢«æ›¿æ¢ä¸º0")
            dino_features = np.nan_to_num(dino_features, nan=0.0)
        
        if inf_count > 0:
            print(f"  è­¦å‘Š: å‘ç° {inf_count} ä¸ªInfå€¼ï¼Œå°†è¢«æ›¿æ¢ä¸ºæœ‰é™å€¼")
            dino_features = np.nan_to_num(dino_features, posinf=1.0, neginf=-1.0)
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœ‰æ•ˆçš„ç‰¹å¾
        if np.all(dino_features == 0):
            print(f"  é”™è¯¯: æ‰€æœ‰ç‰¹å¾éƒ½ä¸º0ï¼Œæ— æ³•è¿›è¡ŒPCA")
            raise ValueError(f"æ ·æœ¬ {sample_id} çš„æ‰€æœ‰DINOv3ç‰¹å¾éƒ½ä¸º0ï¼Œå¯èƒ½æ˜¯ç‰¹å¾æå–å¤±è´¥")
        
        # åŠ¨æ€è°ƒæ•´PCAç»´åº¦ï¼Œä¸èƒ½è¶…è¿‡æ ·æœ¬æ•°
        n_samples = dino_features.shape[0]
        n_features = dino_features.shape[1]
        max_components = min(n_samples, n_features)
        
        actual_dino_dim = min(self.final_dino_dim, max_components - 1)
        
        if actual_dino_dim < self.final_dino_dim:
            print(f"  è­¦å‘Š: PCAç»´åº¦ä» {self.final_dino_dim} è°ƒæ•´ä¸º {actual_dino_dim} (æ ·æœ¬æ•°é™åˆ¶)")
        
        if actual_dino_dim <= 0:
            print(f"  é”™è¯¯: æ— æ³•ç¡®å®šæœ‰æ•ˆçš„PCAç»´åº¦")
            raise ValueError(f"æ ·æœ¬ {sample_id} æ— æ³•ç¡®å®šæœ‰æ•ˆçš„PCAç»´åº¦ï¼Œæ ·æœ¬æ•°={n_samples}, ç‰¹å¾æ•°={n_features}")
        
        # ä¸ºå½“å‰æ ·æœ¬ç‹¬ç«‹è®­ç»ƒPCA
        from sklearn.decomposition import PCA
        try:
            sample_pca = PCA(n_components=actual_dino_dim, random_state=42)
            reduced_features = sample_pca.fit_transform(dino_features)
        except Exception as e:
            print(f"  PCAè®­ç»ƒå¤±è´¥: {e}")
            print(f"  ç‰¹å¾ç»Ÿè®¡: min={dino_features.min():.6f}, max={dino_features.max():.6f}, mean={dino_features.mean():.6f}, std={dino_features.std():.6f}")
            raise ValueError(f"æ ·æœ¬ {sample_id} PCAè®­ç»ƒå¤±è´¥: {e}") from e
        
        # ä¿å­˜å½“å‰æ ·æœ¬çš„PCAæ¨¡å‹
        sample_pca_path = os.path.join(self.output_dir, f"{sample_id}_dino_pca_model.pkl")
        with open(sample_pca_path, 'wb') as f:
            pickle.dump(sample_pca, f)
        
        # è®¡ç®—æ–¹å·®è§£é‡Šæ¯”ä¾‹
        explained_variance = sample_pca.explained_variance_ratio_.sum()
        explained_variance_each = sample_pca.explained_variance_ratio_
        
        print(f"æ ·æœ¬ {sample_id} PCAè®­ç»ƒå®Œæˆ:")
        print(f"  - è¾“å…¥ç»´åº¦: {dino_features.shape[1]}")
        print(f"  - è¾“å‡ºç»´åº¦: {actual_dino_dim}")
        print(f"  - æ€»è§£é‡Šæ–¹å·®æ¯”ä¾‹: {explained_variance:.4f} ({explained_variance*100:.2f}%)")
        
        # æ˜¾ç¤ºå‰10ä¸ªä¸»æˆåˆ†çš„æ–¹å·®è§£é‡Šæ¯”ä¾‹
        print(f"  - å‰10ä¸ªä¸»æˆåˆ†çš„æ–¹å·®è§£é‡Šæ¯”ä¾‹:")
        for i in range(min(10, len(explained_variance_each))):
            print(f"    PC{i+1}: {explained_variance_each[i]:.4f} ({explained_variance_each[i]*100:.2f}%)")
        
        # æ˜¾ç¤ºç´¯ç§¯è§£é‡Šæ¯”ä¾‹
        cumulative_variance = np.cumsum(explained_variance_each)
        print(f"  - ç´¯ç§¯è§£é‡Šæ¯”ä¾‹:")
        milestones = [10, 20, 50, 100, 128]
        for milestone in milestones:
            if milestone <= len(cumulative_variance):
                print(f"    å‰{milestone}ä¸ªä¸»æˆåˆ†: {cumulative_variance[milestone-1]:.4f} ({cumulative_variance[milestone-1]*100:.2f}%)")
        
        print(f"  - PCAæ¨¡å‹ä¿å­˜: {sample_pca_path}")
        
        return reduced_features
    
    
    def save_features(self, sample_id, combined_features, metadata):
        """ä¿å­˜æå–çš„ç‰¹å¾"""
        output_file = os.path.join(self.output_dir, f"{sample_id}_combined_features.npz")
        
        np.savez_compressed(
            output_file,
            features=combined_features,
            metadata=metadata
        )
        
        print(f"ç‰¹å¾å·²ä¿å­˜: {output_file}")
        return output_file



def get_all_hest_samples(hest_data_dir):
    """è·å–æ‰€æœ‰å¯ç”¨çš„HESTæ ·æœ¬ID"""
    import glob
    import os
    
    # æŸ¥æ‰¾æ‰€æœ‰cellvitåˆ†å‰²æ–‡ä»¶
    cellvit_pattern = os.path.join(hest_data_dir, "cellvit_seg", "*_cellvit_seg.parquet")
    cellvit_files = glob.glob(cellvit_pattern)
    
    # æå–æ ·æœ¬ID
    sample_ids = []
    for file_path in cellvit_files:
        filename = os.path.basename(file_path)
        sample_id = filename.replace('_cellvit_seg.parquet', '')
        
        # æ£€æŸ¥å¯¹åº”çš„WSIæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        wsi_path = os.path.join(hest_data_dir, "wsis", f"{sample_id}.tif")
        if os.path.exists(wsi_path):
            sample_ids.append(sample_id)
    
    sample_ids.sort()  # æŒ‰å­—æ¯é¡ºåºæ’åº
    return sample_ids


def get_progress_file(output_dir):
    """è·å–è¿›åº¦æ–‡ä»¶è·¯å¾„"""
    return os.path.join(output_dir, "extraction_progress.json")

def load_progress(output_dir):
    """åŠ è½½å¤„ç†è¿›åº¦"""
    progress_file = get_progress_file(output_dir)
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r') as f:
                progress = json.load(f)
            print(f"âœ“ åŠ è½½è¿›åº¦æ–‡ä»¶: {progress_file}")
            return progress
        except Exception as e:
            print(f"âš ï¸  åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    return {}

def save_progress(output_dir, progress):
    """ä¿å­˜å¤„ç†è¿›åº¦"""
    progress_file = get_progress_file(output_dir)
    try:
        with open(progress_file, 'w') as f:
            json.dump(progress, f, indent=2)
        print(f"âœ“ ä¿å­˜è¿›åº¦: {progress_file}")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

def is_sample_completed(output_dir, sample_id):
    """æ£€æŸ¥æ ·æœ¬æ˜¯å¦å·²å®Œæˆå¤„ç†"""
    output_file = os.path.join(output_dir, f"{sample_id}_combined_features.npz")
    pca_file = os.path.join(output_dir, f"{sample_id}_dino_pca_model.pkl")
    return os.path.exists(output_file) and os.path.exists(pca_file)

def main_independent_pca_extraction():
    """ä¸»å‡½æ•°ï¼šç©ºè½¬æ•°æ®ç‹¬ç«‹PCAç‰¹å¾æå–ï¼ˆæ¯ä¾‹ç‹¬ç«‹PCAï¼‰- æ”¯æŒæ–­ç‚¹ç»­ä¼ """
    
    # é…ç½®å‚æ•°
    hest_data_dir = "/data/yujk/hovernet2feature/HEST/hest_data"
    output_dir = "/data/yujk/hovernet2feature/hest_spatial_features_independent_pca_dinov3"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰å¯ç”¨çš„HESTæ ·æœ¬
    all_samples = get_all_hest_samples(hest_data_dir)
    print(f"\nå‘ç° {len(all_samples)} ä¸ªå¯ç”¨æ ·æœ¬: {all_samples}")
    
    # åŠ è½½å¤„ç†è¿›åº¦
    progress = load_progress(output_dir)
    completed_samples = set(progress.get('completed_samples', []))
    failed_samples = set(progress.get('failed_samples', []))
    
    # æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿä¸­å·²å®Œæˆçš„æ ·æœ¬
    file_completed_samples = set()
    for sample_id in all_samples:
        if is_sample_completed(output_dir, sample_id):
            file_completed_samples.add(sample_id)
    
    # åˆå¹¶è¿›åº¦ä¿¡æ¯
    all_completed = completed_samples.union(file_completed_samples)
    
    # ç¡®å®šéœ€è¦å¤„ç†çš„æ ·æœ¬
    remaining_samples = [s for s in all_samples if s not in all_completed]
    
    print(f"\n=== æ–­ç‚¹ç»­ä¼ çŠ¶æ€ ===")
    print(f"æ€»æ ·æœ¬æ•°: {len(all_samples)}")
    print(f"å·²å®Œæˆæ ·æœ¬: {len(all_completed)} - {sorted(list(all_completed))}")
    print(f"å¤±è´¥æ ·æœ¬: {len(failed_samples)} - {sorted(list(failed_samples))}")
    print(f"å¾…å¤„ç†æ ·æœ¬: {len(remaining_samples)} - {sorted(remaining_samples)}")
    
    if not remaining_samples:
        print("âœ… æ‰€æœ‰æ ·æœ¬å·²å¤„ç†å®Œæˆï¼")
        return
    
    # è¯¢é—®æ˜¯å¦ä»æ–­ç‚¹ç»§ç»­
    if all_completed:
        print(f"\næ£€æµ‹åˆ° {len(all_completed)} ä¸ªå·²å®Œæˆçš„æ ·æœ¬")
        try:
            resume = input("æ˜¯å¦ä»æ–­ç‚¹ç»§ç»­å¤„ç†å‰©ä½™æ ·æœ¬ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower()
            if resume in ['n', 'no']:
                print("ç”¨æˆ·é€‰æ‹©é‡æ–°å¼€å§‹å¤„ç†")
                # é‡ç½®è¿›åº¦
                remaining_samples = all_samples
                progress = {'completed_samples': [], 'failed_samples': []}
                save_progress(output_dir, progress)
        except KeyboardInterrupt:
            print("\nç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return
    
    test_samples = remaining_samples  # åªå¤„ç†å‰©ä½™æ ·æœ¬
    
    print("=== HESTç©ºè½¬æ•°æ®ç‹¬ç«‹PCAç‰¹å¾æå–ï¼ˆDINOv3ï¼‰- æ–­ç‚¹ç»­ä¼ ç‰ˆ ===")
    print(f"æ•°æ®ç›®å½•: {hest_data_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ç‰¹å¾é…ç½®: ä»…DINOv3ï¼Œæ¯ä¾‹ç‹¬ç«‹PCAè‡³128ç»´")
    print(f"ä½¿ç”¨çº§åˆ«1åˆ†è¾¨ç‡WSI (~0.5Î¼m/pixel)")
    print(f"48Ã—48åƒç´ patchesï¼Œè¦†ç›–24Ã—24Î¼mç‰©ç†åŒºåŸŸ")
    
    # åˆ›å»ºç‰¹å¾æå–å™¨ï¼ˆä½¿ç”¨ä¼˜åŒ–å‚æ•°ï¼‰
    try:
        # è‡ªåŠ¨æ£€æµ‹æœ€ä½³å‚æ•°
        num_workers = min(mp.cpu_count(), 16)  # é™åˆ¶æœ€å¤§16ä¸ªçº¿ç¨‹
        
        # GPUå†…å­˜æ£€æµ‹
        if torch.cuda.is_available():
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"æ£€æµ‹åˆ°GPUå†…å­˜: {gpu_memory_gb:.1f} GB")
            
            # æ ¹æ®GPUå†…å­˜è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å¤§å°
            if gpu_memory_gb >= 24:  # 24GB+
                dino_batch_size = 512
                cell_batch_size = 100000
            elif gpu_memory_gb >= 16:  # 16-24GB
                dino_batch_size = 384
                cell_batch_size = 80000
            elif gpu_memory_gb >= 12:  # 12-16GB
                dino_batch_size = 256
                cell_batch_size = 60000
            elif gpu_memory_gb >= 8:   # 8-12GB
                dino_batch_size = 128
                cell_batch_size = 40000
            else:  # <8GB
                dino_batch_size = 64
                cell_batch_size = 20000
        else:
            print("æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
            dino_batch_size = 32
            cell_batch_size = 10000
        
        print(f"è‡ªåŠ¨ä¼˜åŒ–é…ç½®:")
        print(f"  - CPUæ ¸å¿ƒæ•°: {mp.cpu_count()}, ä½¿ç”¨çº¿ç¨‹æ•°: {num_workers}")
        print(f"  - DINOv3æ‰¹å¤§å°: {dino_batch_size}")
        print(f"  - ç»†èƒæ‰¹å¤§å°: {cell_batch_size:,}")
        
        # è®¾ç½® DINOv3 æ¨¡å‹è·¯å¾„
        dinov3_model_path = "/data/yujk/hovernet2feature/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth"
        
        extractor = HESTCellFeatureExtractor(
            hest_data_dir=hest_data_dir,
            output_dir=output_dir,
            dinov3_model_path=dinov3_model_path,  # ä¼ å…¥DINOv3æ¨¡å‹è·¯å¾„
            bulk_pca_model_path=None,  # ä¸ä½¿ç”¨bulk PCA
            device='cuda' if torch.cuda.is_available() else 'cpu',
            dinov3_feature_dim=1024,  # DINOv3-L ç‰¹å¾ç»´åº¦
            dino_batch_size=dino_batch_size,
            cell_batch_size=cell_batch_size,
            num_workers=num_workers
        )
        
        # å¤„ç†æ‰€æœ‰ç©ºè½¬æ ·æœ¬
        print(f"\n=== å¼€å§‹å¤„ç† {len(test_samples)} ä¸ªç©ºè½¬æ ·æœ¬ ===")
        print(f"å‡†å¤‡å¤„ç†æ ·æœ¬: {', '.join(test_samples[:5])}{'...' if len(test_samples) > 5 else ''}")
        
        sample_results = []
        
        # æ€§èƒ½ç›‘æ§
        import time
        start_time = time.time()
        
        # å¤„ç†æ¯ä¸ªæ ·æœ¬ï¼ˆä½¿ç”¨ç‹¬ç«‹PCA + æ–­ç‚¹ç»­ä¼ ï¼‰
        for sample_idx, sample_id in enumerate(test_samples):
            print(f"\næ ·æœ¬å¤„ç†è¿›åº¦: {sample_idx+1}/{len(test_samples)}")
            print(f"{'='*50}")
            print(f"æ­£åœ¨å¤„ç†ç©ºè½¬æ ·æœ¬: {sample_id}")
            print(f"{'='*50}")
            
            # æ£€æŸ¥æ ·æœ¬æ˜¯å¦å·²å®Œæˆ
            if is_sample_completed(output_dir, sample_id):
                print(f"âœ… æ ·æœ¬ {sample_id} å·²å®Œæˆï¼Œè·³è¿‡")
                # æ·»åŠ åˆ°ç»“æœä¸­ä»¥ä¾¿ç»Ÿè®¡
                try:
                    output_file = os.path.join(output_dir, f"{sample_id}_combined_features.npz")
                    data = np.load(output_file)
                    num_cells = data['features'].shape[0]
                    final_feature_dim = data['features'].shape[1]
                    sample_results.append({
                        'sample_id': sample_id,
                        'num_cells': num_cells,
                        'final_feature_dim': final_feature_dim,
                        'output_file': output_file
                    })
                except Exception as e:
                    print(f"âš ï¸  è¯»å–å·²å®Œæˆæ ·æœ¬ {sample_id} ä¿¡æ¯å¤±è´¥: {e}")
                continue
            
            # å¼‚å¸¸å¤„ç†ï¼šå¦‚æœæŸä¸ªæ ·æœ¬å¤„ç†å¤±è´¥ï¼Œè®°å½•å¹¶ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
            try:
                sample_start_time = time.time()
                result = extractor.process_sample_with_independent_pca(sample_id)
                sample_end_time = time.time()
                sample_time = sample_end_time - sample_start_time
                
                sample_results.append(result)
                
                # æ›´æ–°è¿›åº¦ï¼šæ·»åŠ åˆ°å·²å®Œæˆåˆ—è¡¨
                current_progress = load_progress(output_dir)
                if 'completed_samples' not in current_progress:
                    current_progress['completed_samples'] = []
                if sample_id not in current_progress['completed_samples']:
                    current_progress['completed_samples'].append(sample_id)
                
                # ä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if 'failed_samples' in current_progress and sample_id in current_progress['failed_samples']:
                    current_progress['failed_samples'].remove(sample_id)
                
                save_progress(output_dir, current_progress)
                
                print(f"\næ ·æœ¬ {sample_id} å¤„ç†å®Œæˆ (è€—æ—¶: {sample_time:.1f}ç§’):")
                print(f"  - å¤„ç†ç»†èƒæ•°: {result['num_cells']:,}")
                print(f"  - æœ€ç»ˆç‰¹å¾ç»´åº¦: {result['final_feature_dim']}")
                print(f"  - ç‰¹å¾æ–‡ä»¶: {result['output_file']}")
                print(f"  - å¤„ç†é€Ÿåº¦: {result['num_cells']/sample_time:.0f} ç»†èƒ/ç§’")
                
                # å®æ—¶èµ„æºç›‘æ§
                resource_info = extractor.monitor_resources()
                print(f"  - å½“å‰èµ„æºä½¿ç”¨: {resource_info}")
                
            except Exception as e:
                print(f"\nâŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
                print(f"   è·³è¿‡è¯¥æ ·æœ¬ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...")
                import traceback
                print(f"   é”™è¯¯è¯¦æƒ…: {traceback.format_exc()[:300]}...")
                
                # æ›´æ–°è¿›åº¦ï¼šæ·»åŠ åˆ°å¤±è´¥åˆ—è¡¨
                current_progress = load_progress(output_dir)
                if 'failed_samples' not in current_progress:
                    current_progress['failed_samples'] = []
                if sample_id not in current_progress['failed_samples']:
                    current_progress['failed_samples'].append(sample_id)
                save_progress(output_dir, current_progress)
                
                continue  # è·³è¿‡å¤±è´¥çš„æ ·æœ¬ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
        
        # æœ€ç»ˆè¿›åº¦æ›´æ–°
        final_progress = load_progress(output_dir)
        if 'completed_samples' not in final_progress:
            final_progress['completed_samples'] = []
        
        # ç¡®ä¿æ‰€æœ‰æˆåŠŸå¤„ç†çš„æ ·æœ¬éƒ½åœ¨å·²å®Œæˆåˆ—è¡¨ä¸­
        for result in sample_results:
            if result['sample_id'] not in final_progress['completed_samples']:
                final_progress['completed_samples'].append(result['sample_id'])
        
        save_progress(output_dir, final_progress)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸå¤„ç†çš„æ ·æœ¬
        if not sample_results:
            print("\nâŒ æ‰€æœ‰æ ·æœ¬å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return
        
        if len(sample_results) < len(test_samples):
            failed_samples = set(test_samples) - set([r['sample_id'] for r in sample_results])
            print(f"\nâš ï¸  ä»¥ä¸‹æ ·æœ¬å¤„ç†å¤±è´¥: {failed_samples}")
        
        # è®¡ç®—æ€»å¤„ç†æ—¶é—´å’Œæ€§èƒ½ç»Ÿè®¡
        end_time = time.time()
        total_time = end_time - start_time
        total_cells = sum(result['num_cells'] for result in sample_results)
        
        # æ‰¹é‡å¤„ç†æ€§èƒ½æŠ¥å‘Š
        print(f"\n{'='*80}")
        print("=== ç©ºè½¬æ•°æ®ç‹¬ç«‹PCAå¤„ç†å®Œæˆ - æ€§èƒ½æŠ¥å‘Š ===")
        print(f"{'='*80}")
        
        print(f"ğŸ† æ‰¹é‡å¤„ç†ç»Ÿè®¡:")
        print(f"  - æˆåŠŸå¤„ç†æ ·æœ¬æ•°: {len(sample_results)}")
        print(f"  - è·³è¿‡/å¤±è´¥æ ·æœ¬æ•°: {len(test_samples) - len(sample_results)}")
        print(f"  - æˆåŠŸå¤„ç†çš„æ ·æœ¬: {[r['sample_id'] for r in sample_results]}")
        
        print(f"âš¡ æ€§èƒ½ç»Ÿè®¡:")
        print(f"  - æ€»å¤„ç†æ—¶é—´: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        print(f"  - æ€»å¤„ç†ç»†èƒæ•°: {total_cells:,}")
        print(f"  - å¹³å‡å¤„ç†é€Ÿåº¦: {total_cells/total_time:.0f} ç»†èƒ/ç§’")
        print(f"  - æ¯ä¸ªæ ·æœ¬å¹³å‡æ—¶é—´: {total_time/len(sample_results):.1f}ç§’")
        print(f"  - æ¯ä¸ªæ ·æœ¬å¹³å‡ç»†èƒæ•°: {total_cells//len(sample_results):,}")
        
        print(f"ğŸ“Š ä¿å­˜ç»Ÿè®¡:")
        print(f"  - æˆåŠŸä¿å­˜æ ·æœ¬æ•°: {len(sample_results)}/{len(test_samples)}")
        print(f"  - ä¿å­˜æˆåŠŸç‡: {len(sample_results)/len(test_samples)*100:.1f}%")
        
        print(f"âœ… ç‹¬ç«‹PCAé…ç½®:")
        print(f"  - å¤„ç†æ ·æœ¬æ•°: {len(sample_results)}/{len(test_samples)}")
        print(f"  - æ¯ä¸ªç»†èƒç‰¹å¾ç»´åº¦: {extractor.final_feature_dim} (æ ·æœ¬ç‹¬ç«‹PCA)")
        print(f"  - WSIåˆ†è¾¨ç‡: çº§åˆ«1 (~0.5Î¼m/pixel)")
        print(f"  - Patchå¤§å°: {extractor.cell_patch_size}Ã—{extractor.cell_patch_size}åƒç´  (24Ã—24Î¼m)")
        print(f"  - å›¾åƒæ¥æº: ä»…çœŸå®WSIç»†èƒå›¾åƒ")
        print(f"  - è¾“å‡ºç›®å½•: {output_dir}")
        print(f"  - æ€§èƒ½ä¼˜åŒ–: DINOv2æ‰¹å¤§å°{extractor.dino_batch_size}, ç»†èƒæ‰¹å¤§å°{extractor.cell_batch_size:,}, å¤šçº¿ç¨‹{extractor.num_workers}ä¸ª")
        
        print(f"\nğŸ“„ æ ·æœ¬è¯¦ç»†ä¿¡æ¯:")
        for result in sample_results:
            print(f"  - {result['sample_id']}: {result['num_cells']:,}ç»†èƒ, {result['final_feature_dim']}ç»´ç‰¹å¾")
        
        # æœ€ç»ˆèµ„æºä½¿ç”¨ç»Ÿè®¡
        final_resource_info = extractor.monitor_resources()
        print(f"\n  - æœ€ç»ˆèµ„æºä½¿ç”¨: {final_resource_info}")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
        # åˆå§‹åŒ–sample_resultsä»¥é˜²æ­¢UnboundLocalError
        if 'sample_results' not in locals():
            sample_results = []
        if 'test_samples' not in locals():
            test_samples = []
        if 'extractor' not in locals():
            print("â— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
            return
            
        print(f"\nâ— å¤„ç†ä¸­æ–­ï¼Œå½“å‰çŠ¶æ€:")
        print(f"  - å¤„ç†æ ·æœ¬æ•°: {len(sample_results)}/{len(test_samples)}")
        if len(sample_results) > 0:
            print(f"  - æ¯ä¸ªç»†èƒç‰¹å¾ç»´åº¦: {sample_results[0].get('final_feature_dim', 'N/A')}")
        print(f"  - WSIåˆ†è¾¨ç‡: çº§åˆ«1 (~0.5Î¼m/pixel)")
        print(f"  - Patchå¤§å°: 48Ã—48åƒç´  (24Ã—24Î¼m)")
        print(f"  - å›¾åƒæ¥æº: ä»…çœŸå®WSIç»†èƒå›¾åƒ")
        print(f"  - è¾“å‡ºç›®å½•: {output_dir}")
        
        if len(sample_results) > 0:
            print(f"\nğŸ“„ å·²å®Œæˆæ ·æœ¬è¯¦ç»†ä¿¡æ¯:")
            for result in sample_results:
                print(f"  - {result['sample_id']}: {result['num_cells']:,}ç»†èƒ, {result['final_feature_dim']}ç»´ç‰¹å¾")




if __name__ == "__main__":
    # è¿è¡Œç©ºè½¬æ•°æ®ç‹¬ç«‹PCAç‰¹å¾æå–ï¼ˆä»…DINOv2ç‰¹å¾ï¼‰
    print("HESTç»†èƒç‰¹å¾æå–å™¨ - ä»…ä½¿ç”¨DINOv2ç‰¹å¾ï¼ˆç‹¬ç«‹PCAï¼‰")
    print("ç‰¹å¾é…ç½®: DINOv2 768ç»´ -> PCAé™ç»´è‡³128ç»´")
    print("ä¸åŒ…å«å½¢æ€ç‰¹å¾ï¼Œæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹è®­ç»ƒPCA")
    print()
    
    try:
        main_independent_pca_extraction()
    except KeyboardInterrupt:
        print("\nç”¨æˆ·å–æ¶ˆæ“ä½œ")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()